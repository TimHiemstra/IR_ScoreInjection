{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sJQ2Voc-Bqj",
        "outputId": "b10625c8-ab07-45df-acc3-8b57c87e8f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/221.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/221.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n",
            "Collecting cohere\n",
            "  Downloading cohere-4.37-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Installing collected packages: fastavro, backoff, cohere\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.37 fastavro-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4wAgJvr_Q6d",
        "outputId": "ce4af43a-2f58-4be6-c59e-9a22ac49fc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il8TdarVzQp3",
        "outputId": "744050fc-c892-41ad-c1ce-5a410ef2f1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.23.0-py3-none-any.whl (140.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.5.3)\n",
            "Collecting pyjnius>=1.4.0 (from pyserini)\n",
            "  Downloading pyjnius-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.35.2)\n",
            "Collecting sentencepiece>=0.1.95 (from pyserini)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nmslib>=2.1.1 (from pyserini)\n",
            "  Downloading nmslib-2.1.1.tar.gz (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.8.1 (from pyserini)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from pyserini) (4.1.0)\n",
            "Requirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from pyserini) (3.6.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from pyserini) (6.0.1)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyserini) (1.3.7)\n",
            "Collecting tiktoken>=0.4.0 (from pyserini)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11<2.6.2 (from nmslib>=2.1.1->pyserini)\n",
            "  Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nmslib>=2.1.1->pyserini) (5.9.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->pyserini) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->pyserini) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->pyserini) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (2.31.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->pyserini) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.19.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->pyserini) (0.4.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=1.0.0->pyserini) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai>=1.0.0->pyserini) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->pyserini) (0.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers>=4.6.0->pyserini) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.4.0->pyserini) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.8.1->pyserini)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
            "Building wheels for collected packages: nmslib\n",
            "  Building wheel for nmslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nmslib: filename=nmslib-2.1.1-cp310-cp310-linux_x86_64.whl size=13578642 sha256=fd7a327a00b52349d739b168682e6a15b3a4eaa15c673b56447fcbd08c979196\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/1a/5d/4cc754a5b1a88405cad184b76f823897a63a8d19afcd4b9314\n",
            "Successfully built nmslib\n",
            "Installing collected packages: sentencepiece, pyjnius, pybind11, humanfriendly, tiktoken, nmslib, coloredlogs, onnxruntime, pyserini\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nmslib-2.1.1 onnxruntime-1.16.3 pybind11-2.6.1 pyjnius-1.6.1 pyserini-0.23.0 sentencepiece-0.1.99 tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyserini\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone Anserini repository\n",
        "!git clone https://github.com/castorini/anserini.git --recurse-submodules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4aXv80_zd6B",
        "outputId": "99fd6294-36c5-455b-ee97-87c09101a216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'anserini'...\n",
            "remote: Enumerating objects: 31081, done.\u001b[K\n",
            "remote: Counting objects: 100% (5399/5399), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1087/1087), done.\u001b[K\n",
            "remote: Total 31081 (delta 4474), reused 4960 (delta 4152), pack-reused 25682\u001b[K\n",
            "Receiving objects: 100% (31081/31081), 87.26 MiB | 21.92 MiB/s, done.\n",
            "Resolving deltas: 100% (21121/21121), done.\n",
            "Submodule 'tools' (https://github.com/castorini/anserini-tools.git) registered for path 'tools'\n",
            "Cloning into '/content/anserini/tools'...\n",
            "remote: Enumerating objects: 864, done.        \n",
            "remote: Counting objects: 100% (621/621), done.        \n",
            "remote: Compressing objects: 100% (542/542), done.        \n",
            "remote: Total 864 (delta 112), reused 574 (delta 78), pack-reused 243        \n",
            "Receiving objects: 100% (864/864), 212.80 MiB | 16.19 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "Submodule path 'tools': checked out '77b8e58afc461d56f951c77cd9d3650378e121dd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract MSMARCO Passage dataset\n",
        "!wget https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz -P data/msmarco_passage/\n",
        "!tar xvfz data/msmarco_passage/collection.tar.gz -C data/msmarco_passage\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kxi4JDde234s",
        "outputId": "24595122-7af7-4602-8792-d11d67f44a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 18:44:25--  https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz\n",
            "Resolving msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)... 20.150.34.1\n",
            "Connecting to msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)|20.150.34.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1035009698 (987M) [application/octet-stream]\n",
            "Saving to: ‘data/msmarco_passage/collection.tar.gz’\n",
            "\n",
            "collection.tar.gz   100%[===================>] 987.06M  28.3MB/s    in 39s     \n",
            "\n",
            "2023-12-05 18:45:05 (25.1 MB/s) - ‘data/msmarco_passage/collection.tar.gz’ saved [1035009698/1035009698]\n",
            "\n",
            "collection.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert MSMARCO Passage dataset from tsv to jsonl\n",
        "!cd anserini && python tools/scripts/msmarco/convert_collection_to_jsonl.py \\\n",
        " --collection-path ../data/msmarco_passage/collection.tsv --output-folder ../data/msmarco_passage/collection_jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJlLFsds6DWp",
        "outputId": "8635cb33-53da-41a9-8d80-3cfbd6dbaab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting collection...\n",
            "Converted 0 docs, writing into file 1\n",
            "Converted 100,000 docs, writing into file 1\n",
            "Converted 200,000 docs, writing into file 1\n",
            "Converted 300,000 docs, writing into file 1\n",
            "Converted 400,000 docs, writing into file 1\n",
            "Converted 500,000 docs, writing into file 1\n",
            "Converted 600,000 docs, writing into file 1\n",
            "Converted 700,000 docs, writing into file 1\n",
            "Converted 800,000 docs, writing into file 1\n",
            "Converted 900,000 docs, writing into file 1\n",
            "Converted 1,000,000 docs, writing into file 2\n",
            "Converted 1,100,000 docs, writing into file 2\n",
            "Converted 1,200,000 docs, writing into file 2\n",
            "Converted 1,300,000 docs, writing into file 2\n",
            "Converted 1,400,000 docs, writing into file 2\n",
            "Converted 1,500,000 docs, writing into file 2\n",
            "Converted 1,600,000 docs, writing into file 2\n",
            "Converted 1,700,000 docs, writing into file 2\n",
            "Converted 1,800,000 docs, writing into file 2\n",
            "Converted 1,900,000 docs, writing into file 2\n",
            "Converted 2,000,000 docs, writing into file 3\n",
            "Converted 2,100,000 docs, writing into file 3\n",
            "Converted 2,200,000 docs, writing into file 3\n",
            "Converted 2,300,000 docs, writing into file 3\n",
            "Converted 2,400,000 docs, writing into file 3\n",
            "Converted 2,500,000 docs, writing into file 3\n",
            "Converted 2,600,000 docs, writing into file 3\n",
            "Converted 2,700,000 docs, writing into file 3\n",
            "Converted 2,800,000 docs, writing into file 3\n",
            "Converted 2,900,000 docs, writing into file 3\n",
            "Converted 3,000,000 docs, writing into file 4\n",
            "Converted 3,100,000 docs, writing into file 4\n",
            "Converted 3,200,000 docs, writing into file 4\n",
            "Converted 3,300,000 docs, writing into file 4\n",
            "Converted 3,400,000 docs, writing into file 4\n",
            "Converted 3,500,000 docs, writing into file 4\n",
            "Converted 3,600,000 docs, writing into file 4\n",
            "Converted 3,700,000 docs, writing into file 4\n",
            "Converted 3,800,000 docs, writing into file 4\n",
            "Converted 3,900,000 docs, writing into file 4\n",
            "Converted 4,000,000 docs, writing into file 5\n",
            "Converted 4,100,000 docs, writing into file 5\n",
            "Converted 4,200,000 docs, writing into file 5\n",
            "Converted 4,300,000 docs, writing into file 5\n",
            "Converted 4,400,000 docs, writing into file 5\n",
            "Converted 4,500,000 docs, writing into file 5\n",
            "Converted 4,600,000 docs, writing into file 5\n",
            "Converted 4,700,000 docs, writing into file 5\n",
            "Converted 4,800,000 docs, writing into file 5\n",
            "Converted 4,900,000 docs, writing into file 5\n",
            "Converted 5,000,000 docs, writing into file 6\n",
            "Converted 5,100,000 docs, writing into file 6\n",
            "Converted 5,200,000 docs, writing into file 6\n",
            "Converted 5,300,000 docs, writing into file 6\n",
            "Converted 5,400,000 docs, writing into file 6\n",
            "Converted 5,500,000 docs, writing into file 6\n",
            "Converted 5,600,000 docs, writing into file 6\n",
            "Converted 5,700,000 docs, writing into file 6\n",
            "Converted 5,800,000 docs, writing into file 6\n",
            "Converted 5,900,000 docs, writing into file 6\n",
            "Converted 6,000,000 docs, writing into file 7\n",
            "Converted 6,100,000 docs, writing into file 7\n",
            "Converted 6,200,000 docs, writing into file 7\n",
            "Converted 6,300,000 docs, writing into file 7\n",
            "Converted 6,400,000 docs, writing into file 7\n",
            "Converted 6,500,000 docs, writing into file 7\n",
            "Converted 6,600,000 docs, writing into file 7\n",
            "Converted 6,700,000 docs, writing into file 7\n",
            "Converted 6,800,000 docs, writing into file 7\n",
            "Converted 6,900,000 docs, writing into file 7\n",
            "Converted 7,000,000 docs, writing into file 8\n",
            "Converted 7,100,000 docs, writing into file 8\n",
            "Converted 7,200,000 docs, writing into file 8\n",
            "Converted 7,300,000 docs, writing into file 8\n",
            "Converted 7,400,000 docs, writing into file 8\n",
            "Converted 7,500,000 docs, writing into file 8\n",
            "Converted 7,600,000 docs, writing into file 8\n",
            "Converted 7,700,000 docs, writing into file 8\n",
            "Converted 7,800,000 docs, writing into file 8\n",
            "Converted 7,900,000 docs, writing into file 8\n",
            "Converted 8,000,000 docs, writing into file 9\n",
            "Converted 8,100,000 docs, writing into file 9\n",
            "Converted 8,200,000 docs, writing into file 9\n",
            "Converted 8,300,000 docs, writing into file 9\n",
            "Converted 8,400,000 docs, writing into file 9\n",
            "Converted 8,500,000 docs, writing into file 9\n",
            "Converted 8,600,000 docs, writing into file 9\n",
            "Converted 8,700,000 docs, writing into file 9\n",
            "Converted 8,800,000 docs, writing into file 9\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm data/msmarco_passage/*.tsv\n",
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "p0qXxvld6EJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Index the MSMARCO Passage dataset using Pyserini\n",
        "!python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 9 \\\n",
        "-input data/msmarco_passage/collection_jsonl -index indexes/lucene-index-msmarco-passage -storePositions -storeDocvectors -storeRaw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URw5CwfQ6lsQ",
        "outputId": "b634493f-a36b-418c-c1cd-03b030e8d530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2023-12-05 18:53:49,782 INFO  [main] index.IndexCollection (IndexCollection.java:380) - Setting log level to INFO\n",
            "2023-12-05 18:53:49,791 INFO  [main] index.IndexCollection (IndexCollection.java:383) - Starting indexer...\n",
            "2023-12-05 18:53:49,791 INFO  [main] index.IndexCollection (IndexCollection.java:384) - ============ Loading Parameters ============\n",
            "2023-12-05 18:53:49,792 INFO  [main] index.IndexCollection (IndexCollection.java:385) - DocumentCollection path: data/msmarco_passage/collection_jsonl\n",
            "2023-12-05 18:53:49,792 INFO  [main] index.IndexCollection (IndexCollection.java:386) - CollectionClass: JsonCollection\n",
            "2023-12-05 18:53:49,793 INFO  [main] index.IndexCollection (IndexCollection.java:387) - Generator: DefaultLuceneDocumentGenerator\n",
            "2023-12-05 18:53:49,793 INFO  [main] index.IndexCollection (IndexCollection.java:388) - Threads: 9\n",
            "2023-12-05 18:53:49,793 INFO  [main] index.IndexCollection (IndexCollection.java:389) - Language: en\n",
            "2023-12-05 18:53:49,794 INFO  [main] index.IndexCollection (IndexCollection.java:390) - Stemmer: porter\n",
            "2023-12-05 18:53:49,794 INFO  [main] index.IndexCollection (IndexCollection.java:391) - Keep stopwords? false\n",
            "2023-12-05 18:53:49,794 INFO  [main] index.IndexCollection (IndexCollection.java:392) - Stopwords: null\n",
            "2023-12-05 18:53:49,795 INFO  [main] index.IndexCollection (IndexCollection.java:393) - Store positions? true\n",
            "2023-12-05 18:53:49,795 INFO  [main] index.IndexCollection (IndexCollection.java:394) - Store docvectors? true\n",
            "2023-12-05 18:53:49,795 INFO  [main] index.IndexCollection (IndexCollection.java:395) - Store document \"contents\" field? false\n",
            "2023-12-05 18:53:49,796 INFO  [main] index.IndexCollection (IndexCollection.java:396) - Store document \"raw\" field? true\n",
            "2023-12-05 18:53:49,796 INFO  [main] index.IndexCollection (IndexCollection.java:397) - Additional fields to index: []\n",
            "2023-12-05 18:53:49,796 INFO  [main] index.IndexCollection (IndexCollection.java:398) - Optimize (merge segments)? false\n",
            "2023-12-05 18:53:49,797 INFO  [main] index.IndexCollection (IndexCollection.java:399) - Whitelist: null\n",
            "2023-12-05 18:53:49,804 INFO  [main] index.IndexCollection (IndexCollection.java:400) - Pretokenized?: false\n",
            "2023-12-05 18:53:49,805 INFO  [main] index.IndexCollection (IndexCollection.java:401) - Index path: indexes/lucene-index-msmarco-passage\n",
            "2023-12-05 18:53:49,810 INFO  [main] index.IndexCollection (IndexCollection.java:481) - ============ Indexing Collection ============\n",
            "2023-12-05 18:53:49,862 INFO  [main] index.IndexCollection (IndexCollection.java:468) - Using DefaultEnglishAnalyzer\n",
            "2023-12-05 18:53:49,863 INFO  [main] index.IndexCollection (IndexCollection.java:469) - Stemmer: porter\n",
            "2023-12-05 18:53:49,864 INFO  [main] index.IndexCollection (IndexCollection.java:470) - Keep stopwords? false\n",
            "2023-12-05 18:53:49,864 INFO  [main] index.IndexCollection (IndexCollection.java:471) - Stopwords file: null\n",
            "2023-12-05 18:53:50,148 INFO  [main] index.IndexCollection (IndexCollection.java:510) - Thread pool with 9 threads initialized.\n",
            "2023-12-05 18:53:50,149 INFO  [main] index.IndexCollection (IndexCollection.java:512) - Initializing collection in data/msmarco_passage/collection_jsonl\n",
            "2023-12-05 18:53:50,154 INFO  [main] index.IndexCollection (IndexCollection.java:521) - 9 files found\n",
            "2023-12-05 18:53:50,155 INFO  [main] index.IndexCollection (IndexCollection.java:522) - Starting to index...\n",
            "2023-12-05 18:54:50,176 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 440,000 documents indexed\n",
            "2023-12-05 18:55:50,178 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 1,090,000 documents indexed\n",
            "2023-12-05 18:56:50,179 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 1,810,000 documents indexed\n",
            "2023-12-05 18:57:50,190 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 2,480,000 documents indexed\n",
            "2023-12-05 18:58:50,191 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 3,150,000 documents indexed\n",
            "2023-12-05 18:59:50,192 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 3,850,000 documents indexed\n",
            "2023-12-05 19:00:50,193 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 4,500,000 documents indexed\n",
            "2023-12-05 19:01:50,194 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 5,160,000 documents indexed\n",
            "2023-12-05 19:02:50,199 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 5,770,000 documents indexed\n",
            "2023-12-05 19:03:50,201 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 6,330,000 documents indexed\n",
            "2023-12-05 19:04:50,202 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 0.00% of files completed, 6,980,000 documents indexed\n",
            "2023-12-05 19:05:32,530 DEBUG [pool-2-thread-9] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs08.json: 841823 docs added.\n",
            "2023-12-05 19:05:50,207 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 11.11% of files completed, 7,551,823 documents indexed\n",
            "2023-12-05 19:06:50,208 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 11.11% of files completed, 8,111,823 documents indexed\n",
            "2023-12-05 19:07:45,559 DEBUG [pool-2-thread-4] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs07.json: 1000000 docs added.\n",
            "2023-12-05 19:07:48,557 DEBUG [pool-2-thread-2] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs01.json: 1000000 docs added.\n",
            "2023-12-05 19:07:50,210 INFO  [main] index.IndexCollection (IndexCollection.java:536) - 33.33% of files completed, 8,661,823 documents indexed\n",
            "2023-12-05 19:07:52,739 DEBUG [pool-2-thread-3] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs00.json: 1000000 docs added.\n",
            "2023-12-05 19:07:57,593 DEBUG [pool-2-thread-5] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs02.json: 1000000 docs added.\n",
            "2023-12-05 19:07:58,625 DEBUG [pool-2-thread-8] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs03.json: 1000000 docs added.\n",
            "2023-12-05 19:07:59,202 DEBUG [pool-2-thread-7] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs04.json: 1000000 docs added.\n",
            "2023-12-05 19:08:04,154 DEBUG [pool-2-thread-6] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs05.json: 1000000 docs added.\n",
            "2023-12-05 19:08:08,232 DEBUG [pool-2-thread-1] index.IndexCollection$LocalIndexerThread (IndexCollection.java:345) - collection_jsonl/docs06.json: 1000000 docs added.\n",
            "2023-12-05 19:09:44,345 INFO  [main] index.IndexCollection (IndexCollection.java:578) - Indexing Complete! 8,841,823 documents indexed\n",
            "2023-12-05 19:09:44,346 INFO  [main] index.IndexCollection (IndexCollection.java:579) - ============ Final Counter Values ============\n",
            "2023-12-05 19:09:44,346 INFO  [main] index.IndexCollection (IndexCollection.java:580) - indexed:        8,841,823\n",
            "2023-12-05 19:09:44,346 INFO  [main] index.IndexCollection (IndexCollection.java:581) - unindexable:            0\n",
            "2023-12-05 19:09:44,346 INFO  [main] index.IndexCollection (IndexCollection.java:582) - empty:                  0\n",
            "2023-12-05 19:09:44,347 INFO  [main] index.IndexCollection (IndexCollection.java:583) - skipped:                0\n",
            "2023-12-05 19:09:44,347 INFO  [main] index.IndexCollection (IndexCollection.java:584) - errors:                 0\n",
            "2023-12-05 19:09:44,372 INFO  [main] index.IndexCollection (IndexCollection.java:587) - Total 8,841,823 documents indexed in 00:15:54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get top 1000 queries\n",
        "!wget https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz -P data/msmarco_passage/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvwWHRB3_IuR",
        "outputId": "6d44e71a-e0f1-43fc-a88d-05f6186c2446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 19:14:52--  https://msmarco.z22.web.core.windows.net/msmarcoranking/msmarco-passagetest2019-top1000.tsv.gz\n",
            "Resolving msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)... 20.150.34.1\n",
            "Connecting to msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)|20.150.34.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26634062 (25M) [application/x-gzip]\n",
            "Saving to: ‘data/msmarco_passage/msmarco-passagetest2019-top1000.tsv.gz’\n",
            "\n",
            "msmarco-passagetest 100%[===================>]  25.40M  6.36MB/s    in 4.0s    \n",
            "\n",
            "2023-12-05 19:14:56 (6.36 MB/s) - ‘data/msmarco_passage/msmarco-passagetest2019-top1000.tsv.gz’ saved [26634062/26634062]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get and extract the queries\n",
        "!wget https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz -P data/msmarco_passage/\n",
        "!tar xvfz data/msmarco_passage/queries.tar.gz -C data/msmarco_passage"
      ],
      "metadata": {
        "id": "lAedKwkWZhZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further works down here are largily taken from https://github.com/arian-askari/RelevanceCAT/tree/main/compute_injection_score\n",
        "The modification that we have applied is the possibility to also calculate the relevance score with Query Likelihood with Dirichlet Smoothing."
      ],
      "metadata": {
        "id": "ZXfEkMw53QO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#store the queries in a dict with its qid and query entry\n",
        "queries = {}\n",
        "queries_filepath = '/content/data/msmarco_passage/queries.train.tsv'\n",
        "\n",
        "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        qid, query = line.strip().split(\"\\t\")\n",
        "        queries[qid] = query"
      ],
      "metadata": {
        "id": "1Nd_2oYM8KZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the evaluation training query files\n",
        "!wget https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz -P data/msmarco_passage/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt-NvmPg8pjP",
        "outputId": "d65df501-4211-4085-ee2b-20b45fcc35e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-04 22:30:39--  https://sbert.net/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz\n",
            "Resolving sbert.net (sbert.net)... 172.67.180.145, 104.21.67.200, 2606:4700:3031::ac43:b491, ...\n",
            "Connecting to sbert.net (sbert.net)|172.67.180.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz [following]\n",
            "--2023-12-04 22:30:40--  https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz\n",
            "Resolving public.ukp.informatik.tu-darmstadt.de (public.ukp.informatik.tu-darmstadt.de)... 130.83.167.186\n",
            "Connecting to public.ukp.informatik.tu-darmstadt.de (public.ukp.informatik.tu-darmstadt.de)|130.83.167.186|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2313734 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘data/msmarco_passage/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz’\n",
            "\n",
            "msmarco-qidpidtripl 100%[===================>]   2.21M  1.54MB/s    in 1.4s    \n",
            "\n",
            "2023-12-04 22:30:42 (1.54 MB/s) - ‘data/msmarco_passage/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz’ saved [2313734/2313734]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_eval_filepath = '/content/data/msmarco_passage/msmarco-qidpidtriples.rnd-shuf.train-eval.tsv.gz'\n",
        "train_filepath = '/content/data/msmarco_passage/bert_cat_ensemble_msmarcopassage_train_scores_ids.tsv?download=1'"
      ],
      "metadata": {
        "id": "oS7zuc9h-yFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.index.lucene import IndexReader\n",
        "from pyserini import analysis, search\n",
        "from pyserini.pyclass import autoclass"
      ],
      "metadata": {
        "id": "iZtOyQY4LzKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#init the similarity method of lucene search for both bm25 and Dirichlet smoothing\n",
        "index_path = 'indexes/lucene-index-msmarco-passage' #lucene-index.cacm\n",
        "index_reader = IndexReader(index_path)\n",
        "b = 0.68\n",
        "k1 = 0.82\n",
        "similarity_bm25 = autoclass('org.apache.lucene.search.similarities.BM25Similarity')(k1, b)\n",
        "mu = 1000\n",
        "similarity_ql = autoclass('org.apache.lucene.search.similarities.LMDirichletSimilarity')(mu)"
      ],
      "metadata": {
        "id": "2npl1WplIC7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "Jgs2pmATHCog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_v-t2bj5C6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = '/content/data/msmarco_passage/msmarco-passagetest2019-top1000.tsv.gz'"
      ],
      "metadata": {
        "id": "GbFq2E6b_ZHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the relevance score for all queries document pairs into a dict\n",
        "scores = {}\n",
        "with gzip.open(queries, 'rt', encoding='utf-8') as fIn:\n",
        "  for line in tqdm.tqdm(fIn, unit_scale=True):\n",
        "    qid, pid, query, passage = line.strip().split(\"\\t\")\n",
        "    if qid not in scores.keys():\n",
        "        scores[qid] = {}\n",
        "    # Calculate the relevance score between a query and a document with QL, change similarity_ql to similarity_bm25 if relevance scores with bm25 is desired\n",
        "    score = index_reader.compute_query_document_score(pid, query, similarity=similarity_ql)\n",
        "    scores[qid][pid] = score\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJznySIqHZjp",
        "outputId": "50d0eee9-82ea-4e94-bff7-689054ed3106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "190kit [01:14, 2.56kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "#export the scores\n",
        "scores_dict_path = \"/content/data/msmarco_passagetest2019_ql_scores.json\"\n",
        "with open(scores_dict_path, \"w+\") as fp:\n",
        "    json.dump(scores, indent=True, fp = fp)"
      ],
      "metadata": {
        "id": "qESNopqZLe_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip"
      ],
      "metadata": {
        "id": "IfFWsFPQWBFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate or add the relevance score for all queries document pairs from the training file into a dict\n",
        "scores_train = {}\n",
        "with open(train_filepath, 'rt') as fIn:\n",
        "    for line in tqdm.tqdm(fIn, unit_scale=True):\n",
        "        pos_score, neg_score, qid, pos_id, neg_id = line.strip().split(\"\\t\")\n",
        "\n",
        "        if qid not in scores_train.keys():\n",
        "          scores_train[qid] = {}\n",
        "        query = queries[qid]\n",
        "\n",
        "        if pos_id in scores_train[qid].keys():\n",
        "            pos_score = scores_train[qid][pos_id]\n",
        "        else:\n",
        "            pos_score = index_reader.compute_query_document_score(pos_id, query, similarity=similarity_ql) #change to similarity_bm25 for bm25 relevance score\n",
        "            scores_train[qid][pos_id] = pos_score\n",
        "\n",
        "        neg_score = index_reader.compute_query_document_score(neg_id, query, similarity=similarity_ql) #change to similarity_bm25 for bm25 relevance score\n",
        "        scores_train[qid][neg_id] = neg_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgDYiRKcUks1",
        "outputId": "5973762a-f341-4edf-8f53-c30d57d9ecf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "39.8Mit [3:53:14, 2.84kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate or add the relevance score for all queries document pairs from the eval file into a dict\n",
        "scores = {}\n",
        "with gzip.open(train_eval_filepath, 'rt') as fIn:\n",
        "    for line in fIn:\n",
        "        qid, pos_id, neg_id = line.strip().split()\n",
        "\n",
        "        if qid not in scores.keys():\n",
        "          scores[qid] = {}\n",
        "        query = queries[qid]\n",
        "\n",
        "        if pos_id in scores[qid].keys():\n",
        "            pos_score = scores[qid][pos_id]\n",
        "        else:\n",
        "            pos_score = index_reader.compute_query_document_score(pos_id, query, similarity=similarity_ql) #change to similarity_bm25 for bm25 relevance score\n",
        "            scores[qid][pos_id] = pos_score\n",
        "\n",
        "        neg_score = index_reader.compute_query_document_score(neg_id, query, similarity=similarity_ql) #change to similarity_bm25 for bm25 relevance score\n",
        "        scores[qid][neg_id] = neg_score"
      ],
      "metadata": {
        "id": "kIOUINXJWQFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export the scores\n",
        "scores_dict_path = \"/content/data/ql_scores_train_triples_small.json\"\n",
        "with open(scores_dict_path, \"w+\") as fp:\n",
        "    json.dump(scores_train, indent=True, fp = fp)"
      ],
      "metadata": {
        "id": "asWAB9EucULs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export the scores\n",
        "scores_dict_path = \"/content/data/ql_scores_train-eval_triples.json\"\n",
        "with open(scores_dict_path, \"w+\") as fp:\n",
        "    json.dump(scores, indent=True, fp = fp)"
      ],
      "metadata": {
        "id": "0jhZ7J8UYez3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}